---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<center class="dune-quote" style="font-size:14px">
<i> 
  "A process cannot be understood by stopping it. Understanding must move with the flow of the process, must join it and flow with it"
</i> <br>
 - First Law of Mentat, Dune
</center>
<br>

<p style="text-align: justify">
I am a PhD candidate in the <a href="https://baulab.info/" target="_blank">Interpretable Neural Networks</a> lab at Northeastern University. I am fortunate to be advised by <a href="https://www.khoury.northeastern.edu/people/david-bau/" target="_blank">Prof. David Bau</a>.
</p>

<!-- <p style="text-align: justify">
Broadly, I am interested in understanding the inner workings of large language models. Specifically, my research focuses on understanding what factual informations (<a href="https://en.wikipedia.org/wiki/Bayesian_network" target="_blank">belief</a>s about the real world) the LMs has learned, how LMs store this information in their parameters, how this information is retrieved during inference to inform their decision making process. Also, I am interested in how this findings can help us detect bugs (bias, false/outdated associations) in LMs and develop tools to steer their behavior with the goal of making them more reliable.
</p> -->

<!-- <p style="text-align: justify">
My research focuses on understanding what factual informations (<a href="https://en.wikipedia.org/wiki/Bayesian_network" target="_blank">belief</a>s about the real world) a language model has learned, how the LM stores this information in its parameters, and how this information is retrieved. Below are some of the key questions I am exploring:
</p> -->

<p style="text-align: justify">
How do language models (or AI systems in general) perform complex reasoning tasks? Do they break down a complex problem into smaller steps, like a programmer implementing an algorithm? If so: how modular and composable are these small reasoning steps? How are they implemented in the model's internals? &mdash; these are some of key questions I am exploring in my research.
</p>

<p style="text-align: justify">
Broadly, I apply tools from <a href="https://arxiv.org/pdf/2410.09087" target="_blank">mechanistic</a> interpretability to form faithful abstractions of the inner workings of LLMs. I am interested in how such insights can help us address specific failure modes on AI systems by designing edits/interventions that target the right abstractions.
</p>

<!-- <p style="text-align: justify">
I apply tools from <a href="https://arxiv.org/pdf/2410.09087" target="_blank">mechanistic</a> interpretability to form faithful abstractions of the inner workings of LLMs. I focus on studying LLMs' <strong>knowledge</strong> &mdash; <a href="https://en.wikipedia.org/wiki/Bayesian_network" target="_blank">belief</a>s about the real world they have learned during training &mdash; how this knowledge is stored in the model parameters, how it is encoded in the model representations during retrieval, and if LLMs can <strong>reason</strong> with this knowledge in scenarios where it needs to link together multiple pieces of information. Broadly, I am interested in how understanding the inner mechanisms of LLMs can help us detect bugs (bias, false/outdated associations) in LLMs and develop tools to steer their behavior with the goal of making them more reliable.
</p> -->

<!-- <p style="text-align: justify">
My research focuses on understanding how LLMs <strong>reason</strong> with the <strong>knowledge</strong> (<a href="https://en.wikipedia.org/wiki/Bayesian_network" target="_blank">belief</a>s about the real world) they have learned during training phase. I apply tools from mechanistic interpretability to understand 
</p>

* <p style="text-align: justify">How to edit a factual association with appropriate entailments?</p>
* <p style="text-align: justify">How can we enable the LMs to do <a href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow" target="_blank">System-2</a> reasoning? Is CoT enough or the only way?</p> -->



<p style="text-align: justify">
Before starting my PhD, I was a Software Engineer at <a href="https://research.samsung.com/" target="_blank">Samsung Research</a> and used to teach at <a href="https://www.sust.edu/d/cse/faculty-profile-detail/700" target="_blank">SUST</a>, from where I have completed my B.Sc. in Computer Science and Engineering.
</p>

<p style="text-align: justify"><i>
Feel free to reach out to me if you would like to chat about research, collaboration, or if you have any questions about my work.
</i></p>


# News

<!-- * [October-7-2024] Attending MATS 7.0 -->

<table style="border: none; font-size: 16px;">

<tr>
  <td style="text-align: left; vertical-align: top; padding-right: 20px; border: none;">
    <strong>10/2025</strong>
  </td>
  <td style="border: none; text-align: left;">
    Serving as a reviewer for ICLR 2026.
  </td>
</tr>

<tr>
  <td style="text-align: left; vertical-align: top; padding-right: 20px; border: none;">
    <strong>08/2025</strong>
  </td>
  <td style="border: none; text-align: left;">
    Attended <a href="https://nemiconf.github.io/summer25/">The 2nd New England Mechanistic Interpretability (NEMI) workshop</a> in Boston.
  </td>
</tr>

<tr>
  <td style="text-align: left; vertical-align: top; padding-right: 20px; border: none;">
    <strong>07/2025</strong>
  </td>
  <td style="border: none; text-align: left;">
    Serving as a reviewer for NeurIPS 2026.
  </td>
</tr>

<tr>
  <td style="text-align: left; vertical-align: top; padding-right: 20px; border: none;">
    <strong>06/2025</strong>
  </td>
  <td style="border: none; text-align: left;">
    <!-- <ul> -->
      <li>Our new <a href="https://arxiv.org/abs/2505.14685">paper</a> investigates how LMs track the mental states of different characters in a story. Experiments show that LMs use a mechanism similar to the double pointers (**) in C++! Checkout Nikhil's <a href="https://x.com/nikhil07prakash/status/1937542573641597335">Twitter thread</a> for more details.</li>
      <li>Serving as a reviewer for NeurIPS 2025.</li>
    <!-- </ul> -->
  </td>
</tr>

<tr>
  <td style="text-align: left; vertical-align: top; padding-right: 20px; border: none;">
    <strong>05/2025</strong>
  </td>
  <td style="border: none; text-align: left;">
    Serving as a reviewer for COLM 2025.
  </td>
</tr>

<tr>
  <td style="text-align: left; vertical-align: top; padding-right: 20px; border: none;">
    <strong>04/2025</strong>
  </td>
  <td style="border: none; text-align: left;">
    Attended <a href="https://nenlp.github.io/spr2025/">NENLP 2025</a> at Yale.
  </td>
</tr>

<tr>
  <td style="text-align: left; vertical-align: top; padding-right: 20px; border: none;">
    <strong>03/2025</strong>
  </td>
  <td style="border: none; text-align: left;">
    Serving as a reviewer for ICML 2025.
  </td>
</tr>

<tr>
  <td style="text-align: left; vertical-align: top; padding-right: 20px; border: none;">
    <strong>01/2025</strong>
  </td>
  <td style="border: none; text-align: left;">
    The <a href="https://openreview.net/forum?id=MxbEiFRf39">NNsight and NDIF</a> paper is out! Super excited about <a href="https://ndif.us/">NDIF</a>s mission to enable interpretability research on very large neural networks.
  </td>
</tr>


<tr>
  <td style="text-align: left; vertical-align: top; padding-right: 20px; border: none;">
    <strong>09/2024</strong>
  </td>
  <td style="border: none; text-align: left;">
    Serving as a reviewer for ICLR 2025.
  </td>
</tr>

<tr>
  <td style="text-align: left; vertical-align: top; padding-right: 20px; border: none;">
    <strong>08/2024</strong>
  </td>
  <td style="border: none; text-align: left;">
    Interpretability researchers are still trying to understand what is the right level of abstraction for conceptualizing neural network computations. Our <a href="https://arxiv.org/pdf/2408.01416">new survay paper</a> proposes a perspective this grounded on causal mediation analysis.
  </td>
</tr>


<tr><td style="text-align: left; vertical-align: top; padding-right: 20px; border: none;"><strong>07/2024</strong></td><td style="border: none; text-align: left;">Serving as a reviewer for NeurIPS 2024. Excited to see many interesting works on interpretability, some of them directly building upon works from our lab!</td></tr>
<tr><td style="text-align: left; vertical-align: top; padding-right: 20px; border: none;"><strong>06/2024</strong></td><td style="border: none; text-align: left;">Serving as a reviewer for COLM 2024.</td></tr>
<tr><td style="text-align: left; vertical-align: top; padding-right: 20px; border: none;"><strong>04/2024</strong></td><td style="border: none; text-align: left;">New paper, <a href="https://arxiv.org/pdf/2404.03646">Locating and Editing Factual Associations in Mamba</a>. <a href="https://github.com/state-spaces/mamba">Mamba</a> is a new generation of sequence modeling architecture that achives per-parameter performance with Transformers in multiple modalities, including language modeling. With the development of such novel architectures, we interpretability researchers must ask - To what extent our insights on certain mechanism (at a high-level) generalize across different architectures? This paper is a case study where we apply the tools developed for understanding and editing factual associations in Transformers to Mamba and check if the insights generalize. Fine more at <a href="https://romba.baulab.info/">project page</a>, <a href="https://github.com/arnab-api/romba">[code]</a>. (Update: Accepted at <strong>COLM 2024</strong>!)</td></tr>
<tr><td style="text-align: left; vertical-align: top; padding-right: 20px; border: none;"><strong>10/2023</strong></td><td style="border: none; text-align: left;">Another paper! <a href="https://arxiv.org/pdf/2310.15213.pdf">Function Vectors in Large Language Models</a>. In this cool paper we show that LLMs encode <em>functions</em> (input-output mappings under a relation, or for performing a certain task, like translation) as a vector in their representation. Checkout <a href="https://twitter.com/ericwtodd/status/1717277426873766104">this</a> Twitter thread for more information. (Update: Accepted at <strong>ICLR 2024</strong>!)</td></tr>
<tr><td style="text-align: left; vertical-align: top; padding-right: 20px; border: none;"><strong>08/2023</strong></td><td style="border: none; text-align: left;">New paper! <a href="https://browse.arxiv.org/pdf/2308.09124.pdf">Linearity of Relation Decoding in Transformer LMs</a>. In this paper we show that for a subset of relations LLMs (highly non-linear) relation decoding procedure can be well-approximated by a single linear transformation (LRE) on the subject representation after some intermediate layer. And this LRE can be achived by constructing a first-order approximation to the LLM computation from a single input. Fine more at <a href="https://lre.baulab.info/">project page</a>, <a href="https://github.com/evandez/relations">[code]</a> (Update: Accepted at <strong>ICLR 2024</strong>!)</td></tr>
<tr><td style="text-align: left; vertical-align: top; padding-right: 20px; border: none;"><strong>01/2023</strong></td><td style="border: none; text-align: left;">Our paper <a href="https://memit.baulab.info/">Mass-Editing Memory in a Transformer</a> has been accepted at <strong>ICLR 2023</strong> (top 25%)!</td></tr>
<tr><td style="text-align: left; vertical-align: top; padding-right: 20px; border: none;"><strong>10/2022</strong></td><td style="border: none; text-align: left;">New paper! <a href="https://memit.baulab.info/">Mass-Editing Memory in a Transformer</a>. Here we scale up <a href="https://rome.baulab.info/">ROME</a> to edit upto 10K memories in a LLM. Find more at <a href="https://memit.baulab.info/">project page</a>.</td></tr>
<tr><td style="text-align: left; vertical-align: top; padding-right: 20px; border: none;"><strong>09/2022</strong></td><td style="border: none; text-align: left;">Starting my PhD at Northeastern University, Boston. I will be working with <a href="https://www.khoury.northeastern.edu/people/david-bau/">Prof. David Bau</a> on interpretability of LLMs.</td></tr>
</table>